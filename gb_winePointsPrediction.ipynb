{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 754: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e40156f4b415>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mfeatures1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"price\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#\"country\", \"vintage\", \"province\", \"region_1\", \"variety\", top1_desc, top2_desc, top3_desc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mcollected_data1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_to_lst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[0mfeatures2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"taster_name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e40156f4b415>\u001b[0m in \u001b[0;36mfile_to_lst\u001b[1;34m(filename, features)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# NOT USED HERE: ID (0), designation (2), title (8)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mcollected_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mcollected_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mINDEX_POINTS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 754: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "    \n",
    "# READ THE CSV FILE         \n",
    "\n",
    "'''-------------- file_to_lst ---------------------------\n",
    "IN: the csv filename where the data are stored\n",
    "    features_INDEX: list of the indexes of the features \n",
    "    we want to keep \n",
    "RESULT: the data in a list\n",
    "-------------------------------------------------------'''\n",
    "\n",
    "def file_to_lst(filename, features):\n",
    "    file = open(filename, \"r\")\n",
    "    reader = csv.reader(file, delimiter=\",\")\n",
    "    \n",
    "    # INDEX DEFINITION \n",
    "    name_columns = next(reader)\n",
    "\n",
    "    INDEX_POINTS = name_columns.index(\"points\") \n",
    "    \n",
    "    features_INDEX = []\n",
    "    for i, feature in enumerate(features):\n",
    "        features_INDEX.append(name_columns.index(feature))\n",
    "        \n",
    "        \n",
    "    collected_data = []\n",
    "\n",
    "    # NOT USED HERE: ID (0), designation (2), title (8)\n",
    "    \n",
    "    for r in reader:\n",
    "        collected_data.append([])\n",
    "        collected_data[-1].append(ast.literal_eval(r[INDEX_POINTS]))\n",
    "        for i, index in enumerate(features_INDEX):\n",
    "            feature_value = ast.literal_eval(r[index])\n",
    "            try: \n",
    "                for j, value in enumerate(feature_value):\n",
    "                    collected_data[-1].append(value)\n",
    "            except TypeError:\n",
    "                collected_data[-1].append(feature_value)\n",
    "                \n",
    "            \n",
    "    file.close()\n",
    "    return collected_data\n",
    "\n",
    "######################################\n",
    "#              CALL\n",
    "######################################\n",
    "\n",
    "#filename = \"DF_version2Francesco.csv\"\n",
    "#filename = \"DF_version2_withDesc.csv\"\n",
    "#filename = \"DF_version2.csv\"\n",
    "filename = \"test1.csv\"\n",
    "\n",
    "features1 = [\"price\"] #\"country\", \"vintage\", \"province\", \"region_1\", \"variety\", top1_desc, top2_desc, top3_desc\n",
    "collected_data1 = file_to_lst(filename, features1)\n",
    "\n",
    "features2 = [\"taster_name\"]\n",
    "collected_data2 = file_to_lst(filename, features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collected_data1[1])\n",
    "print(len(collected_data1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "\n",
    "# FORMAT THE DATA (from list to np-array)\n",
    "                               \n",
    "'''-------------------- format_data -------------------------\n",
    "IN: collected_data: list of a list reflecting the data (where\n",
    "the last element is what it has to be predicted)\n",
    "RESULT: numpy array to be used for training/testing a model\n",
    " ----------------------------------------------------------'''\n",
    "\n",
    "\n",
    "def format_data(collected_data):\n",
    "    size = len(collected_data[0]) -1\n",
    "    x = np.zeros((len(collected_data), size), dtype=np.float)\n",
    "    y = np.zeros((x.shape[0],), dtype=np.float)\n",
    "\n",
    "    for k, data in enumerate(collected_data):\n",
    "\n",
    "        x[k] = data[1:]\n",
    "        y[k] = data[0]\n",
    "\n",
    "    return x, y                               \n",
    "                               \n",
    "\n",
    "    \n",
    "# SPLIT THE DATASET INTO A TRAINING AND A TESTING SET \n",
    "def DS_builder(collected_data, p=0.75):\n",
    "\n",
    "    # Shuffle the data to reduce the bias\n",
    "    random.shuffle(collected_data)\n",
    "    \n",
    "    data_len = len(collected_data)\n",
    "    testing_set = []\n",
    "    learning_set = []\n",
    "    \n",
    "    # ------ We go trough all the data ------\n",
    "    for count, data in enumerate(collected_data):\n",
    "            if count / data_len < p:\n",
    "                learning_set.append(data)\n",
    "            else:\n",
    "                testing_set.append(data)\n",
    "\n",
    "    xls, yls = format_data(learning_set)\n",
    "    xts, yts = format_data(testing_set)\n",
    "    ds = [xls, yls, xts, yts]\n",
    "    \n",
    "    return ds\n",
    "\n",
    "INDEX_XLS = 0\n",
    "INDEX_YLS = 1 \n",
    "INDEX_XTS = 2\n",
    "INDEX_YTS = 3\n",
    "\n",
    "######################################\n",
    "#              CALL\n",
    "######################################\n",
    "\n",
    "ds1 = DS_builder(collected_data1)\n",
    "ds2 = DS_builder(collected_data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# STILL TODO: parameter tuning \n",
    "########################################\n",
    "\n",
    "\n",
    "# DEFINE YOUR MODEL AND TRAIN IT\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR \n",
    "\n",
    "algorithm = \"LinearRegression\"\n",
    "\n",
    "\n",
    "# ------------ Management of the learning algorithm ----------------------\n",
    "if algorithm == \"LinearRegression\":\n",
    "    estimator1 = LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None).fit(ds1[INDEX_XLS], ds1[INDEX_YLS])\n",
    "    estimator2 = LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None).fit(ds2[INDEX_XLS], ds2[INDEX_YLS])\n",
    "\n",
    "elif algorithm == \"LogisticRegression\":\n",
    "    estimator1 = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None).fit(ds1[INDEX_XLS], ds1[INDEX_YLS])\n",
    "    estimator2 = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='warn', max_iter=100, multi_class='warn', verbose=0, warm_start=False, n_jobs=None).fit(ds2[INDEX_XLS], ds2[INDEX_YLS])\n",
    "\n",
    "elif algorithm == \"DecisionTreeRegressor\":\n",
    "    estimator1 = DecisionTreeRegressor(max_depth=5).fit(ds1[INDEX_XLS], ds1[INDEX_YLS])\n",
    "    estimator2 = DecisionTreeRegressor(max_depth=5).fit(ds2[INDEX_XLS], ds2[INDEX_YLS])\n",
    "\n",
    "elif algorithm == \"SVR\":\n",
    "    svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1).fit(ds1[INDEX_XLS], ds1[INDEX_YLS])\n",
    "    svr_lin = SVR(kernel='linear', C=1e3).fit(ds1[INDEX_XLS], ds1[INDEX_YLS])\n",
    "    svr_poly = SVR(kernel='poly', C=1e3, degree=2).fit(ds1[INDEX_XLS], ds1[INDEX_YLS])\n",
    "\n",
    "else:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "# STILL TODO: Interpretation of the computed accuracy (and maybe test with other metrics)\n",
    "###########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "with_prob = False\n",
    "\n",
    "# MAKE PREDICTION \n",
    "if with_prob:\n",
    "    predicted_points1 = estimator1.predict_proba(ds1[INDEX_XTS])\n",
    "else: \n",
    "    predicted_points1 = estimator1.predict(ds1[INDEX_XTS])\n",
    "    predicted_points2 = estimator2.predict(ds2[INDEX_XTS])\n",
    "\n",
    "    \n",
    "# COMPUTE ACCURACY \n",
    "print(\"Test 1 with the features: \" + str(features1) + \" and the algorithm \" + algorithm + \"\\n\")\n",
    "print(\"Test 2 with the features: \" + str(features2) + \" and the algorithm \" + algorithm + \"\\n\")\n",
    "\n",
    "evs1 = explained_variance_score(ds1[INDEX_YTS], predicted_points1)\n",
    "print(\"The explained variance score is: \" + str(evs1))\n",
    "\n",
    "evs2 = explained_variance_score(ds2[INDEX_YTS], predicted_points2)\n",
    "print(\"The explained variance score is: \" + str(evs2) + \"\\n\")\n",
    "\n",
    "mae1 = mean_absolute_error(ds1[INDEX_YTS], predicted_points1) # The best value is 0 \n",
    "print(\"The mean absolute error is: \" + str(mae1))\n",
    "\n",
    "mae2 = mean_absolute_error(ds2[INDEX_YTS], predicted_points2) # The best value is 0 \n",
    "print(\"The mean absolute error is: \" + str(mae2) + \"\\n\")\n",
    "\n",
    "mse1 = mean_squared_error(ds1[INDEX_YTS], predicted_points1) # The best value is 0 \n",
    "print(\"The mean squared error is: \" + str(mae1))\n",
    "\n",
    "mse2 = mean_squared_error(ds2[INDEX_YTS], predicted_points2) # The best value is 0 \n",
    "print(\"The mean squared error is: \" + str(mae2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_points1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
