{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "data = pd.read_csv(\"dataAfterPOS.csv\")\n",
    "stemmedData = pd.read_csv(\"stemmed.csv\")\n",
    "data['description'] = data['description'].str.replace('[^\\w\\s]','')\n",
    "stemmedData['description'] = stemmedData['description'].str.replace('[^\\w\\s]','')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing (always good for everything you do): MUST do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work on data NOT stemmed\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "data[\"stopwords\"] = data[\"description\"].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "\n",
    "#remove most frequent and less frequent bcs useless\n",
    "freq = pd.Series(' '.join(data['description']).split()).value_counts()[:10]\n",
    "freq = list(freq.index)\n",
    "data['description'] = data['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "unfreq = pd.Series(' '.join(data[\"description\"]).split()).value_counts()\n",
    "count = 0\n",
    "for el in unfreq: \n",
    "    if(el < 10):\n",
    "        count = count + 1\n",
    "        \n",
    "unfreqList = unfreq[-count:]\n",
    "unfreq = dict(unfreqList)  \n",
    "data[\"description\"] = data['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in unfreq))\n",
    "\n",
    "data.drop([\"word_count\",\"stopwords\"],axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work on STEMMED data\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "stemmedData[\"stopwords\"] = stemmedData[\"description\"].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "\n",
    "#remove most frequent and less frequent bcs useless\n",
    "freq = pd.Series(' '.join(stemmedData['description']).split()).value_counts()[:10]\n",
    "freq = list(freq.index)\n",
    "stemmedData['description'] = stemmedData['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "unfreq = pd.Series(' '.join(stemmedData[\"description\"]).split()).value_counts()\n",
    "count = 0\n",
    "for el in unfreq: \n",
    "    if(el < 10):\n",
    "        count = count + 1\n",
    "        \n",
    "unfreqList = unfreq[-count:]\n",
    "unfreq = dict(unfreqList)  \n",
    "stemmedData[\"description\"] = stemmedData['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in unfreq))\n",
    "\n",
    "stemmedData.drop([\"word_count\",\"stopwords\"],axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf + idf + tf-idf on whole vanilla dataset (no stemming = \"basic\" words) !!!need to run one of the first two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullText = pd.Series(' '.join(data['description']).split()).value_counts()\n",
    "#getValueCounts\n",
    "tfData = {}\n",
    "for idx, row in data.iterrows(): \n",
    "    tfData[idx] = pd.Series(row['description'].split()).value_counts();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute tf for every word\n",
    "tf1 = {}\n",
    "for idx in tfData.keys():\n",
    "    tf1[idx] = {}\n",
    "    for word in tfData[idx].keys():\n",
    "        tf1[idx][word] = tfData[idx][word] / fullText[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = []\n",
    "for idx, row in data.iterrows():\n",
    "    modded = []\n",
    "    temp = row[\"description\"].split()\n",
    "    for word in temp:\n",
    "        modded.append(tf1[idx][word])\n",
    "    desc.append(modded)\n",
    "#desc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"description_values_tf\"] = desc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf\n",
    "docCount = dict.fromkeys(fullText.keys(),0)\n",
    "idfDict1 = {}\n",
    "N = len(data)                                #numero di documenti #dizionario contenente tutte le parole presenti \n",
    "idfData = dict(tfData)\n",
    "for el in idfData:     #per ogni descrizione\n",
    "    for word,val in idfData[el].items():\n",
    "        docCount[word] = docCount[word] + 1\n",
    "        \n",
    "for word in fullText.keys():\n",
    "    idfDict1[word] = math.log10(N/docCount[word])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute tf-idf\n",
    "tfIdf1 = {}\n",
    "for idx in tfData.keys():\n",
    "    tfIdf1[idx] = {}\n",
    "    for word in tfData[idx].keys():\n",
    "        val = tf1[idx][word] * idfDict1[word]\n",
    "        tfIdf1[idx][word] = val\n",
    "#tfIdf1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc1 = []\n",
    "for idx, row in data.iterrows():\n",
    "    modded = []\n",
    "    temp = row[\"description\"].split()\n",
    "    for word in temp:\n",
    "        modded.append(tfIdf1[idx][word])\n",
    "    desc1.append(modded)\n",
    "#desc1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"description_values_tfIdf\"] = desc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data_tf_tfIdf_fullDataset.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf + idf + tf_idf on whole dataset (stemmed) !!! need to run one of the first two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullText = pd.Series(' '.join(stemmedData['description']).split()).value_counts()\n",
    "#getValueCounts\n",
    "tfData = {}\n",
    "for idx, row in stemmedData.iterrows(): \n",
    "    tfData[idx] = pd.Series(row['description'].split()).value_counts();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute tf for every word\n",
    "tf1 = {}\n",
    "for idx in tfData.keys():\n",
    "    tf1[idx] = {}\n",
    "    for word in tfData[idx].keys():\n",
    "        tf1[idx][word] = tfData[idx][word] / fullText[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = []\n",
    "for idx, row in stemmedData.iterrows():\n",
    "    modded = []\n",
    "    temp = row[\"description\"].split()\n",
    "    for word in temp:\n",
    "        modded.append(tf1[idx][word])\n",
    "    desc.append(modded)\n",
    "#desc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmedData[\"description_values_tf\"] = desc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf\n",
    "docCount = dict.fromkeys(fullText.keys(),0)\n",
    "idfDict1 = {}\n",
    "N = len(stemmedData)                                #numero di documenti #dizionario contenente tutte le parole presenti \n",
    "idfData = dict(tfData)\n",
    "for el in idfData:     #per ogni descrizione\n",
    "    for word,val in idfData[el].items():\n",
    "        docCount[word] = docCount[word] + 1\n",
    "        \n",
    "for word in fullText.keys():\n",
    "    idfDict1[word] = math.log10(N/docCount[word])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute tf-idf\n",
    "tfIdf1 = {}\n",
    "for idx in tfData.keys():\n",
    "    tfIdf1[idx] = {}\n",
    "    for word in tfData[idx].keys():\n",
    "        val = tf1[idx][word] * idfDict1[word]\n",
    "        tfIdf1[idx][word] = val\n",
    "#tfIdf1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc1 = []\n",
    "for idx, row in stemmedData.iterrows():\n",
    "    modded = []\n",
    "    temp = row[\"description\"].split()\n",
    "    for word in temp:\n",
    "        modded.append(tfIdf1[idx][word])\n",
    "    desc1.append(modded)\n",
    "#desc1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmedData[\"description_values_tfIdf\"] = desc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmedData.to_csv(\"data_tf_tfIdf_stemmedDataset.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf + tfidf on stemmed dataset grouped by variety (run all below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"stemmed.csv\")\n",
    "#remove punctuation\n",
    "data['description'] = data['description'].str.replace('[^\\w\\s]','')\n",
    "data[\"word_count\"] = data[\"description\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "data = data[data[\"word_count\"] >= 3] \n",
    "\n",
    "#remove most frequent words, not influent bcs very frequent\n",
    "freq = pd.Series(' '.join(data['description']).split()).value_counts()[:10]\n",
    "\n",
    "freq = list(freq.index)\n",
    "data['description'] = data['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "data[\"stopwords\"] = data[\"description\"].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "\n",
    "unfreq = pd.Series(' '.join(data[\"description\"]).split()).value_counts()\n",
    "count = 0\n",
    "for el in unfreq: \n",
    "    if(el < 10):\n",
    "        count = count + 1\n",
    "print(count)   #elementi poco frequenti sotto il 10 sono spesso typos/parole unite o robe brutte\n",
    "\n",
    "unfreqList = unfreq[-count:]\n",
    "#unfreqList\n",
    "\n",
    "unfreq = dict(unfreqList)  #con dictionary is veeeeeeeeeeeery fasteroni\n",
    "data[\"description\"] = data['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in unfreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullText = pd.Series(' '.join(data['description']).split()).value_counts()\n",
    "\n",
    "totalWords = pd.Series(\"\".join(data[\"description\"]).split()).value_counts().sum()\n",
    "print(totalWords)\n",
    "\n",
    "t1 = data.groupby([\"variety\"])\n",
    "t2 = pd.DataFrame({'text' : t1['description'].apply(lambda x: (x + \" \").sum())}).reset_index()\n",
    "t2[\"word_count\"] = t2[\"text\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "t2 = t2.sort_values('word_count',ascending = False)\n",
    "#t2.iloc[0][\"text\"]\n",
    "#t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varietyDict = {}\n",
    "for idx, row in t2.iterrows():\n",
    "    varietyDict[row[\"variety\"]] = pd.Series(row['text'].split()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfDict = {}\n",
    "for var in varietyDict:\n",
    "    temp = {}\n",
    "    for word in varietyDict[var].keys():\n",
    "        temp[word] = varietyDict[var][word] / fullText[word]   #compute tf= numero di occorrenze della parola nel documento /numero di occorrenze della stessa parola in tutti i documenti \n",
    "    tfDict[var] = temp\n",
    "#tfDict[\"Pinot Gris\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varList = data[\"variety\"].unique().tolist()\n",
    "\n",
    "def computeIdf1(dataset):\n",
    "    a = []\n",
    "    idf = {}\n",
    "    for idx,row in dataset.iterrows():\n",
    "        el = row[\"variety\"]\n",
    "        docList = data[data[\"variety\"] == el]\n",
    "        N = len(docList)\n",
    "        idfDict = {}\n",
    "        temp = t2[t2[\"variety\"] == el]\n",
    "        idfDict = dict.fromkeys(temp.iloc[0][\"text\"].split(),0)\n",
    "        \n",
    "        for idx,row in docList.iterrows():\n",
    "            for word in row[\"description\"].split():\n",
    "                idfDict[word] += 1\n",
    "        \n",
    "        for word, val in idfDict.items():\n",
    "            idfDict[word] = math.log10(N/float(val))\n",
    "        a.append([el,N,idfDict])    \n",
    "        idf[el] = idfDict\n",
    "    return idf, a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf,a = computeIdf1(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfIdf = {}\n",
    "for var in varietyDict:\n",
    "    temp = {}\n",
    "    for word in tfDict[var].keys():\n",
    "        temp[word] = tfDict[var][word] * idf[var][word]\n",
    "    tfIdf[var] = temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "dictlist = []\n",
    "for word,value in tfIdf[\"Chardonnay\"].items():\n",
    "    temp = [word,value]\n",
    "    dictlist.append(temp)\n",
    "dictlist.sort(key=lambda x: x[1],reverse=True)\n",
    "dictlist\n",
    "#con lo stemming cambia un po' le cose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc1 = []\n",
    "for idx, row in data.iterrows():\n",
    "    modded = []\n",
    "    temp = row[\"description\"].split()\n",
    "    for word in temp:\n",
    "        modded.append(tfIdf[row[\"variety\"]][word])\n",
    "    desc1.append(modded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"description_tfIdf\"] = desc1\n",
    "data.drop([\"region_2\",\"word_count\",\"stopwords\"],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data_varietytfIdf_stemmedData.csv\", encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
