{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply description pre processing\n",
    "\n",
    "#arguments {stemming = [True, False]}\n",
    "def preProcess(stemming):\n",
    "    if(stemming):\n",
    "        data = pd.read_csv(\"stemmed.csv\")\n",
    "    elif( not stemming):\n",
    "        data = pd.read_csv(\"dataAfterPOS.csv\")\n",
    "    data.drop([\"region_2\"],inplace=True,axis=1)\n",
    "    #remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    data[\"stopwords\"] = data[\"description\"].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "    \n",
    "    #remove commas, fullstops...\n",
    "    data['description'] = data['description'].str.replace('[^\\w\\s]','')\n",
    "    \n",
    "    #remove most frequent and less frequent bcs useless\n",
    "    freq = pd.Series(' '.join(data['description']).split()).value_counts()[:10]\n",
    "    freq = list(freq.index)\n",
    "    data['description'] = data['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "    unfreq = pd.Series(' '.join(data[\"description\"]).split()).value_counts()\n",
    "    count = 0\n",
    "    for el in unfreq: \n",
    "        if(el < 10):\n",
    "            count = count + 1\n",
    "\n",
    "    unfreqList = unfreq[-count:]\n",
    "    unfreq = dict(unfreqList)  \n",
    "    data[\"description\"] = data['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in unfreq))\n",
    "\n",
    "    data.drop([\"word_count\",\"stopwords\"],axis=1, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeData(data,groupedByVariety,stemming):\n",
    "        if groupedByVariety:\n",
    "            #take only the description with 3 words\n",
    "            fullText = pd.Series(' '.join(data['description']).split()).value_counts()\n",
    "            totalWords = pd.Series(\"\".join(data[\"description\"]).split()).value_counts().sum()\n",
    "            print(totalWords)\n",
    "\n",
    "            t1 = data.groupby([\"variety\"])\n",
    "            t2 = pd.DataFrame({'text' : t1['description'].apply(lambda x: (x + \" \").sum())}).reset_index()\n",
    "            t2[\"word_count\"] = t2[\"text\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "            t2 = t2.sort_values('word_count',ascending = False)\n",
    "            #t2.iloc[0][\"text\"]\n",
    "            #t2\n",
    "            varietyDict = {}\n",
    "            for idx, row in t2.iterrows():\n",
    "                varietyDict[row[\"variety\"]] = pd.Series(row['text'].split()).value_counts()\n",
    "            tfDict = {}\n",
    "            for var in varietyDict:\n",
    "                temp = {}\n",
    "                for word in varietyDict[var].keys():\n",
    "                    temp[word] = varietyDict[var][word] / fullText[word]   #compute tf= numero di occorrenze della parola nel documento /numero di occorrenze della stessa parola in tutti i documenti \n",
    "                tfDict[var] = temp\n",
    "            #tfDict[\"Pinot Gris\"] \n",
    "            varList = data[\"variety\"].unique().tolist()\n",
    "\n",
    "            def computeIdf1(dataset):\n",
    "                a = []\n",
    "                idf = {}\n",
    "                for idx,row in dataset.iterrows():\n",
    "                    el = row[\"variety\"]\n",
    "                    docList = data[data[\"variety\"] == el]\n",
    "                    N = len(docList)\n",
    "                    idfDict = {}\n",
    "                    temp = t2[t2[\"variety\"] == el]\n",
    "                    idfDict = dict.fromkeys(temp.iloc[0][\"text\"].split(),0)\n",
    "\n",
    "                    for idx,row in docList.iterrows():\n",
    "                        for word in row[\"description\"].split():\n",
    "                            idfDict[word] += 1\n",
    "\n",
    "                    for word, val in idfDict.items():\n",
    "                        idfDict[word] = math.log10(N/float(val))\n",
    "                    a.append([el,N,idfDict])    \n",
    "                    idf[el] = idfDict\n",
    "                return idf, a\n",
    "\n",
    "            idf,a = computeIdf1(t2)\n",
    "\n",
    "            tfIdf = {}\n",
    "            for var in varietyDict:\n",
    "                temp = {}\n",
    "                for word in tfDict[var].keys():\n",
    "                    temp[word] = tfDict[var][word] * idf[var][word]\n",
    "                tfIdf[var] = temp \n",
    "\n",
    "            desc1 = []\n",
    "            for idx, row in data.iterrows():\n",
    "                modded = []\n",
    "                temp = row[\"description\"].split()\n",
    "                for word in temp:\n",
    "                    modded.append(tfIdf[row[\"variety\"]][word])\n",
    "                desc1.append(modded)\n",
    "\n",
    "            data[\"description_tfIdf\"] = desc1\n",
    "            if stemming:\n",
    "                data.to_csv(\"stemmedData_tfIdf_groupedByVariety.csv\", encoding='utf-8', index=False)\n",
    "            else: \n",
    "                data.to_csv(\"notStemmedData_tfIdf_groupedByVariety.csv\", encoding='utf-8', index=False)\n",
    "    #if we want to work on the sum of the descriptions as a whole document\n",
    "        elif not groupByVariety:\n",
    "            fullText = pd.Series(' '.join(data['description']).split()).value_counts()\n",
    "            #getValueCounts\n",
    "            tfData = {}\n",
    "            for idx, row in data.iterrows(): \n",
    "                tfData[idx] = pd.Series(row['description'].split()).value_counts();\n",
    "\n",
    "            #compute tf for every word\n",
    "            tf1 = {}\n",
    "            for idx in tfData.keys():\n",
    "                tf1[idx] = {}\n",
    "                for word in tfData[idx].keys():\n",
    "                    tf1[idx][word] = tfData[idx][word] / fullText[word]\n",
    "\n",
    "            desc = []\n",
    "            for idx, row in data.iterrows():\n",
    "                modded = []\n",
    "                temp = row[\"description\"].split()\n",
    "                for word in temp:\n",
    "                    modded.append(tf1[idx][word])\n",
    "                desc.append(modded)\n",
    "\n",
    "            data[\"description_values_tf\"] = desc \n",
    "\n",
    "            #idf\n",
    "            docCount = dict.fromkeys(fullText.keys(),0)\n",
    "            idfDict1 = {}\n",
    "            N = len(data)                                #numero di documenti #dizionario contenente tutte le parole presenti \n",
    "            idfData = dict(tfData)\n",
    "            for el in idfData:     #per ogni descrizione\n",
    "                for word,val in idfData[el].items():\n",
    "                    docCount[word] = docCount[word] + 1\n",
    "\n",
    "            for word in fullText.keys():\n",
    "                idfDict1[word] = math.log10(N/docCount[word])  \n",
    "\n",
    "            #compute tf-idf\n",
    "            tfIdf1 = {}\n",
    "            for idx in tfData.keys():\n",
    "                tfIdf1[idx] = {}\n",
    "                for word in tfData[idx].keys():\n",
    "                    val = tf1[idx][word] * idfDict1[word]\n",
    "                    tfIdf1[idx][word] = val\n",
    "            #tfIdf1[0]\n",
    "\n",
    "            desc1 = []\n",
    "            for idx, row in data.iterrows():\n",
    "                modded = []\n",
    "                temp = row[\"description\"].split()\n",
    "                for word in temp:\n",
    "                    modded.append(tfIdf1[idx][word])\n",
    "                desc1.append(modded)\n",
    "            #desc1[0]\n",
    "            data[\"description_values_tfIdf\"] = desc1\n",
    "            if stemming:\n",
    "                data.to_csv(\"stemmedData_tf_tfIdf_fullDataset.csv\", encoding='utf-8', index=False)\n",
    "            else: \n",
    "                data.to_csv(\"notStemmedData_tf_tfIdf_fullDataset.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = True #if True apply stemming \n",
    "data = preProcess(stemming)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupByVariety = True #if true apply grouping by variety for the description analysis\n",
    "computeData(data,groupByVariety,stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv(\"stemmedData_tfIdf_groupedByVariety.csv\")\n",
    "#pd.read_csv(\"notStemmedData_tfIdf_groupedByVariety.csv\")\n",
    "#pd.read_csv(\"stemmedData_tf_tfIdf_fullDataset.csv\")\n",
    "#pd.read_csv(\"notStemmedData_tf_tfIdf_fullDataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
