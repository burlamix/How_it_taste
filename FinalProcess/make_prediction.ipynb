{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression,Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score,precision_recall_fscore_support,classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import scipy as sp\n",
    "import sklearn as sk # data mining tools\n",
    "import warnings\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117652, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['country', 'description', 'points', 'price', 'taster_name', 'word_count']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wine_base = pd.read_csv(\"Our_dataset/stemmed.csv\", index_col=0)\n",
    "wine_base = pd.read_csv(\"Our_dataset/dataAfterPOS.csv\", index_col=0)\n",
    "#wine_base = pd.read_csv(\"Our_dataset/StemmedWord2vecTop3_parsed_weather_labeled.csv\", index_col=0)\n",
    "#wine_base = pd.read_csv(\"Our_dataset/winemag-data-130k-v2.csv\", index_col=0)\n",
    "wine_base = wine_base.reset_index()\n",
    "wine_base= wine_base[pd.notnull(wine_base['description'])]\n",
    "wine_base= wine_base.drop(['designation','region_2','title','province','variety','winery','region_1'], axis=1)\n",
    "print(wine_base.shape)\n",
    "list(wine_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110033"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "117652\n",
    "110033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_base = wine_base[pd.notnull(wine_base['country'])]\n",
    "wine_base = wine_base[pd.notnull(wine_base['taster_name'])]\n",
    "wine_base = wine_base.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHotEncode(dataframe,column_to_encode,take_whole_dataset=True):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc.fit(dataframe[[column_to_encode]])\n",
    "    mapping = {}\n",
    "    i=0\n",
    "    for elem in enc.categories_[0]:\n",
    "        mapping[elem]=i\n",
    "        i+=1\n",
    "    resu = enc.transform(dataframe[[column_to_encode]]).toarray()\n",
    "    if take_whole_dataset:\n",
    "        for elem in mapping:\n",
    "            dataframe[elem]=resu[:,mapping[elem]]\n",
    "        return dataframe\n",
    "    else:\n",
    "        subset = dataframe[[column_to_encode]]\n",
    "        for elem in mapping:\n",
    "            subset[elem]=resu[:,mapping[elem]]\n",
    "        return subset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_base = OneHotEncode(wine_base,'country')\n",
    "wine_base = wine_base.drop('country', 1)\n",
    "wine_base = OneHotEncode(wine_base,'taster_name')\n",
    "wine_base = wine_base.drop('taster_name', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>word_count</th>\n",
       "      <th>Argentina</th>\n",
       "      <th>Australia</th>\n",
       "      <th>Austria</th>\n",
       "      <th>Chile</th>\n",
       "      <th>France</th>\n",
       "      <th>Germany</th>\n",
       "      <th>...</th>\n",
       "      <th>Kerin O’Keefe</th>\n",
       "      <th>Lauren Buzzeo</th>\n",
       "      <th>Matt Kettmann</th>\n",
       "      <th>Michael Schachner</th>\n",
       "      <th>Paul Gregutt</th>\n",
       "      <th>Roger Voss</th>\n",
       "      <th>Sean P. Sullivan</th>\n",
       "      <th>Susan Kostrzewa</th>\n",
       "      <th>Virginie Boone</th>\n",
       "      <th>unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aromas tropical fruit broom brimstone herb pal...</td>\n",
       "      <td>87</td>\n",
       "      <td>17.0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ripe fruity wine firm tannins juicy red berry ...</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  points  price  \\\n",
       "0  aromas tropical fruit broom brimstone herb pal...      87   17.0   \n",
       "1  ripe fruity wine firm tannins juicy red berry ...      87   15.0   \n",
       "\n",
       "   word_count  Argentina  Australia  Austria  Chile  France  Germany   ...     \\\n",
       "0          24        0.0        0.0      0.0    0.0     0.0      0.0   ...      \n",
       "1          39        0.0        0.0      0.0    0.0     0.0      0.0   ...      \n",
       "\n",
       "   Kerin O’Keefe  Lauren Buzzeo  Matt Kettmann  Michael Schachner  \\\n",
       "0            1.0            0.0            0.0                0.0   \n",
       "1            0.0            0.0            0.0                0.0   \n",
       "\n",
       "   Paul Gregutt  Roger Voss  Sean P. Sullivan  Susan Kostrzewa  \\\n",
       "0           0.0         0.0               0.0              0.0   \n",
       "1           0.0         1.0               0.0              0.0   \n",
       "\n",
       "   Virginie Boone  unknown  \n",
       "0             0.0      0.0  \n",
       "1             0.0      0.0  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_base.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTROL PANEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification or regressor?\n",
    "classificat = False\n",
    "\n",
    "#select only CountVectorizer_data or TfidfVectorizer_data o none, but NOT the two togheder\n",
    "CountVectorizer_data=True\n",
    "TfidfVectorizer_data=False\n",
    "\n",
    "#here select the actibute that you want to use during the training\n",
    "#features=['price','country','taster_name','word_count']\n",
    "#features= list(X_train)[1:]\n",
    "features=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine_base.drop(['points'], axis=1)\n",
    "Y = wine_base['points'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 24385\n"
     ]
    }
   ],
   "source": [
    "if(CountVectorizer_data |TfidfVectorizer_data ):\n",
    "    \n",
    "    if(CountVectorizer_data):\n",
    "        vect = CountVectorizer(min_df=5)\n",
    "        vect.fit(X_train['description'])\n",
    "        print(\"vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "        X_train_vectored_cv = vect.transform(X_train['description'])\n",
    "        X_train_final = X_train_vectored_cv\n",
    "    else:\n",
    "        vect = TfidfVectorizer(smooth_idf=True, sublinear_tf=False, analyzer='word')\n",
    "        vect.fit(X_train['description'])\n",
    "        print(\"vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "        X_train_vectored_tfidf = vect.transform(X_train['description'])\n",
    "        X_train_final = X_train_vectored_tfidf\n",
    "        \n",
    "    for feature in features:\n",
    "        X_train_final = hstack((X_train_final,np.array(X_train[feature])[:,None]))\n",
    "else:\n",
    "    \n",
    "    X_train_final = X_train.loc[:,features]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training error Test error for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression,Ridge,Lasso,SGDRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr = LinearRegression()\n",
    "lr = Ridge()\n",
    "#lr = Lasso()\n",
    "#lr = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "#lr = DecisionTreeRegressor(max_depth=40)\n",
    "lr.fit(X_train_final, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on train\n",
      "1.6624887106841553\n",
      "on test\n",
      "1.8345282312733282\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = lr.predict(X_train_final)\n",
    "if(classificat):\n",
    "    print(classification_report(y_train, y_train_pred, target_names=labels))\n",
    "else:\n",
    "    mse = mean_squared_error(y_train, y_train_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(\"on train\")\n",
    "    print(rmse)\n",
    "if(CountVectorizer_data | TfidfVectorizer_data ):\n",
    "    X_test_final = vect.transform(X_test['description'])\n",
    "    for feature in features:\n",
    "        X_test_final = hstack((X_test_final,np.array(X_test[feature])[:,None]))\n",
    "else:\n",
    "    X_test_final = X_test.loc[:,features]\n",
    "\n",
    "y_test_pred = lr.predict(X_test_final)\n",
    "if(classificat):\n",
    "    print(classification_report(y_test, y_test_pred, target_names=labels))\n",
    "\n",
    "else:\n",
    "    mse = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(\"on test\")\n",
    "    print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['odd' 'acceptable' 'virginia' 'strange' 'weird' 'ammonia' 'york' 'thin'\n",
      " 'weedy' 'skunky' 'clumsy' 'simple' 'watery' 'disappointing' 'vegetal'\n",
      " 'rotten' 'dull' 'weak' 'mealy' 'weediness' 'lacks' 'flat' 'lack'\n",
      " 'uninspired' 'slack' 'torrontes' 'mexico' 'sundae' 'artificial' 'sugary'\n",
      " 'michigan' 'treacly' 'seyval' 'gummy' 'underdeveloped' 'harsh'\n",
      " 'chambourcin' 'chlorophyll' 'flaws' 'gluey' 'vinegar' 'sauerkraut' 'mute'\n",
      " 'short' 'lipped' 'cuisine' 'malleable' 'volatile' 'distract' 'globular']\n",
      "\n",
      "Largest Coefs: \n",
      "['stunning' 'beautiful' 'superb' 'decades' 'sample' 'gorgeous' 'cayuse'\n",
      " 'beauty' 'amazing' 'magnificent' 'extraordinary' 'fabulous' 'stunner'\n",
      " '2030' 'immense' 'greatest' 'impressive' 'endless' 'brilliant' 'rancio'\n",
      " 'spectacular' 'memorable' 'tremendous' 'complex' 'awesome' 'wonderful'\n",
      " 'wow' 'triumph' 'auslese' 'spätlese' 'terrific' 'glorious' 'opulent'\n",
      " 'dreamy' 'peat' 'decade' 'exquisite' 'finest' 'beautifully' 'exceptional'\n",
      " 'remarkable' 'memory' 'impeccable' 'delicious' 'thrill' 'dramatic'\n",
      " 'splendid' 'astonishing' 'knockout']\n"
     ]
    }
   ],
   "source": [
    "# Sort the coefficients from the model (only work for sparse matrix in their own)\n",
    "sorted_coef_index = lr.coef_.argsort()\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# so the list returned is in order of largest to smallest\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:50]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-50:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whith no min term frequency \n",
    "Smallest Coefs:\n",
    "['odd' 'acceptable' 'virginia' 'strange' 'weird' 'ammonia' 'york' 'thin'\n",
    " 'weedy' 'skunky' 'clumsy' 'simple' 'watery' 'disappointing' 'vegetal'\n",
    " 'rotten' 'dull' 'weak' 'mealy' 'weediness' 'lacks' 'flat' 'lack'\n",
    " 'uninspired' 'slack' 'torrontes' 'mexico' 'sundae' 'artificial' 'sugary'\n",
    " 'michigan' 'treacly' 'seyval' 'gummy' 'underdeveloped' 'harsh'\n",
    " 'chambourcin' 'chlorophyll' 'flaws' 'gluey' 'vinegar' 'sauerkraut' 'mute'\n",
    " 'short' 'lipped' 'cuisine' 'malleable' 'volatile' 'distract' 'globular']\n",
    "\n",
    "Largest Coefs: \n",
    "['stunning' 'beautiful' 'superb' 'decades' 'sample' 'gorgeous' 'cayuse'\n",
    " 'beauty' 'amazing' 'magnificent' 'extraordinary' 'fabulous' 'stunner'\n",
    " '2030' 'immense' 'greatest' 'impressive' 'endless' 'brilliant' 'rancio'\n",
    " 'spectacular' 'memorable' 'tremendous' 'complex' 'awesome' 'wonderful'\n",
    " 'wow' 'triumph' 'auslese' 'spätlese' 'terrific' 'glorious' 'opulent'\n",
    " 'dreamy' 'peat' 'decade' 'exquisite' 'finest' 'beautifully' 'exceptional'\n",
    " 'remarkable' 'memory' 'impeccable' 'delicious' 'thrill' 'dramatic'\n",
    " 'splendid' 'astonishing' 'knockout']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print coefficent for tree\n",
    "\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "feature_names\n",
    "feature_imp = lr.feature_importances_\n",
    "\n",
    "sorted_coef_index = feature_imp.argsort()\n",
    "\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-300:-1]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_imp[sorted_coef_index[:-30:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic feature                                              2.859065103992044\n",
    "#basic feature + manual wordcount                           2.5488092414557646\n",
    "#basic feature + word2vect                                  2.7374282971613084\n",
    "#basic feature + weather                                    2.817176644052167\n",
    "#basic feature + tf_grouped                                 2.8167889079501047\n",
    "#basic feature + tf_fullData                                2.820394158650923\n",
    "#basic feature + tfIdf_grouped                              2.800941096964716\n",
    "#basic feature + tfIdf_fullData_1                           2.823836138011456        \n",
    "#basic feature + tfidf                                      1.8287619171940812\n",
    "#basic feature + sklearn word count                         1.8273230684121524  \n",
    "#(this show that tfidf and sklearn word counttwo are almost equivalent so i just use one of that)\n",
    "#since sklean wordcount is the best i check the possible improvement on that\n",
    "\n",
    "#basic feature + sklearn word count + manual word count     1.8269534098049776    +0.001  +-\n",
    "#basic feature + sklearn word count + word2vect             1.8175582513592656    +0.01   +-\n",
    "#basic feature + sklearn word count + weater                1.8143524722884636    +0.01   +-\n",
    "#basic feature + sklearn word count + weater + word2vect    1.8066107753300489    +0.02   +-\n",
    "#bac fe + skl word ct + wear + word t + manual word count   1.8065145709633865    +0.02   ++-\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My conclusion:\n",
    "\n",
    "\n",
    "both the custom tfidf and tf grouped get a better result that on the all dataset.\n",
    "the sklean wordsount and tfidf are way better that the other feature, and they are very similar\n",
    "\n",
    "both word2vect and weater information add new information on the data,\n",
    "the word2vect add much more infomration that weather on the basic feature, but in the basic information + word count the improvement is almost the same.\n",
    "\n",
    "for the manual wordcount as i sensed add much more infomation (compared to word2vect and weather) on the basic feature, but they add very few information on the basic feature + sklean word count, meaning that they close to be a sub set of infomation of the sklean word count.\n",
    "\n",
    "the best think to do it will be check with a p-test the significance of the improvement obtained.\n",
    "to check if a 0.02 is significant same for 0.01 0.001\n",
    "\n",
    "the best will be also to do the same test as i did with another much comples model\n",
    "to validate it\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
