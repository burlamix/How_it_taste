{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vintage</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>...</th>\n",
       "      <th>pr_5</th>\n",
       "      <th>pr_6</th>\n",
       "      <th>pr_7</th>\n",
       "      <th>pr_8</th>\n",
       "      <th>pr_9</th>\n",
       "      <th>tas_5</th>\n",
       "      <th>tas_6</th>\n",
       "      <th>tas_7</th>\n",
       "      <th>tas_8</th>\n",
       "      <th>tas_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1952</td>\n",
       "      <td>8</td>\n",
       "      <td>year ii oak deep oldgold color rich sweet wood...</td>\n",
       "      <td>95</td>\n",
       "      <td>499.0</td>\n",
       "      <td>164</td>\n",
       "      <td>1201</td>\n",
       "      <td>13</td>\n",
       "      <td>376</td>\n",
       "      <td>10276</td>\n",
       "      <td>...</td>\n",
       "      <td>120.181</td>\n",
       "      <td>54.8968</td>\n",
       "      <td>13.3656</td>\n",
       "      <td>12.3677</td>\n",
       "      <td>71.6485</td>\n",
       "      <td>16.0018</td>\n",
       "      <td>20.1771</td>\n",
       "      <td>21.6355</td>\n",
       "      <td>21.3967</td>\n",
       "      <td>18.5723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1952</td>\n",
       "      <td>8</td>\n",
       "      <td>astonish age rich toffe warm intens concentr d...</td>\n",
       "      <td>96</td>\n",
       "      <td>415.0</td>\n",
       "      <td>164</td>\n",
       "      <td>1201</td>\n",
       "      <td>13</td>\n",
       "      <td>376</td>\n",
       "      <td>1703</td>\n",
       "      <td>...</td>\n",
       "      <td>120.181</td>\n",
       "      <td>54.8968</td>\n",
       "      <td>13.3656</td>\n",
       "      <td>12.3677</td>\n",
       "      <td>71.6485</td>\n",
       "      <td>16.0018</td>\n",
       "      <td>20.1771</td>\n",
       "      <td>21.6355</td>\n",
       "      <td>21.3967</td>\n",
       "      <td>18.5723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   vintage  country                                        description  \\\n",
       "0     1952        8  year ii oak deep oldgold color rich sweet wood...   \n",
       "1     1952        8  astonish age rich toffe warm intens concentr d...   \n",
       "\n",
       "   points  price  province  region_1  taster_name  variety  winery   ...     \\\n",
       "0      95  499.0       164      1201           13      376   10276   ...      \n",
       "1      96  415.0       164      1201           13      376    1703   ...      \n",
       "\n",
       "      pr_5     pr_6     pr_7     pr_8     pr_9    tas_5    tas_6    tas_7  \\\n",
       "0  120.181  54.8968  13.3656  12.3677  71.6485  16.0018  20.1771  21.6355   \n",
       "1  120.181  54.8968  13.3656  12.3677  71.6485  16.0018  20.1771  21.6355   \n",
       "\n",
       "     tas_8    tas_9  \n",
       "0  21.3967  18.5723  \n",
       "1  21.3967  18.5723  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_base = pd.read_csv(\"Our_dataset/StemmedWord2vecTop3_parsed_weather_labeled.csv\", index_col=0)\n",
    "#wine_base = pd.read_csv(\"Our_dataset/winemag-data-130k-v2.csv\", index_col=0)\n",
    "wine_base = wine_base.reset_index()\n",
    "wine_base= wine_base[pd.notnull(wine_base['description'])]\n",
    "wine_base.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine_base.drop(['points'], axis=1)\n",
    "Y = wine_base['points'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vintage',\n",
       " 'country',\n",
       " 'description',\n",
       " 'price',\n",
       " 'province',\n",
       " 'region_1',\n",
       " 'taster_name',\n",
       " 'variety',\n",
       " 'winery',\n",
       " 'similarityTop3WinesByVariety',\n",
       " 'word_count',\n",
       " 'tf_grouped_1',\n",
       " 'tf_grouped_2',\n",
       " 'tf_grouped_3',\n",
       " 'tfIdf_grouped_1',\n",
       " 'tfIdf_grouped_2',\n",
       " 'tfIdf_grouped_3',\n",
       " 'tf_fullData_1',\n",
       " 'tf_fullData_2',\n",
       " 'tf_fullData_3',\n",
       " 'tfIdf_fullData_1',\n",
       " 'tfIdf_fullData_2',\n",
       " 'tfIdf_fullData_3',\n",
       " 'pr_5',\n",
       " 'pr_6',\n",
       " 'pr_7',\n",
       " 'pr_8',\n",
       " 'pr_9',\n",
       " 'tas_5',\n",
       " 'tas_6',\n",
       " 'tas_7',\n",
       " 'tas_8',\n",
       " 'tas_9']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X_train)\n",
    "\n",
    "#list of attribute that you can select to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only CountVectorizer_data or TfidfVectorizer_data o none, but NOT the two togheder\n",
    "CountVectorizer_data=True\n",
    "TfidfVectorizer_data=False\n",
    "\n",
    "#here select the actibute that you want to use during the training\n",
    "features=['price','vintage','country','region_1','taster_name','winery','variety','province','similarityTop3WinesByVariety', 'pr_5', 'pr_6', 'pr_7', 'pr_8', 'pr_9', 'tas_5', 'tas_6', 'tas_7', 'tas_8', 'tas_9','word_count']\n",
    "features=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 4772\n"
     ]
    }
   ],
   "source": [
    "if(CountVectorizer_data |TfidfVectorizer_data ):\n",
    "    \n",
    "    if(CountVectorizer_data):\n",
    "        vect = CountVectorizer(min_df=5,ngram_range=(1, 3))\n",
    "        vect.fit(X_train['description'])\n",
    "        print(\"vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "        X_train_vectored_cv = vect.transform(X_train['description'])\n",
    "        X_train_final = X_train_vectored_cv\n",
    "    else:\n",
    "        vect = TfidfVectorizer(smooth_idf=True, sublinear_tf=False, analyzer='word',min_df=50, ngram_range=(1, 3))\n",
    "        vect.fit(X_train['description'])\n",
    "        print(\"vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "        X_train_vectored_tfidf = vect.transform(X_train['description'])\n",
    "        X_train_final = X_train_vectored_tfidf\n",
    "        \n",
    "    for feature in features:\n",
    "        X_train_final = hstack((X_train_final,np.array(X_train[feature])[:,None]))\n",
    "else:\n",
    "    \n",
    "    X_train_final = X_train.loc[:,features]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train_final, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9026960047751693"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred = lr.predict(X_train_final)\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.999243281454173"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if(CountVectorizer_data | TfidfVectorizer_data ):\n",
    "    X_test_final = vect.transform(X_test['description'])\n",
    "    for feature in features:\n",
    "        X_test_final = hstack((X_test_final,np.array(X_test[feature])[:,None]))\n",
    "else:\n",
    "    X_test_final = X_test.loc[:,features]\n",
    "\n",
    "y_test_pred = lr.predict(X_test_final)\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [ 0.56856127 -0.8906486   0.27548894 ... -1.08512404 -0.06398397\n",
      " -0.63492357]\n",
      "4772\n"
     ]
    }
   ],
   "source": [
    "print('Coefficients: \\n', lr.coef_)\n",
    "\n",
    "\n",
    "print(len( lr.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['05', '06', '5050', ..., 'zip', 'zippi', 'zweigelt'], dtype='<U28')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['vinho verd' 'dark cook' 'gala appl' 'woodland berri' 'sauvignon petit'\n",
      " 'touriga nacion' 'accept' 'simpl' 'wateri' 'strang']\n",
      "\n",
      "Largest Coefs: \n",
      "['dark cook spice' 'barrel sampl' 'verd' 'woodland' 'gala' 'stun'\n",
      " 'walla walla' 'superb' 'now2025' 'nacion']\n"
     ]
    }
   ],
   "source": [
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = lr.coef_.argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# so the list returned is in order of largest to smallest\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic feature                                              2.859065103992044\n",
    "#basic feature + manual wordcount                           2.5488092414557646\n",
    "#basic feature + word2vect                                  2.7374282971613084\n",
    "#basic feature + weather                                    2.817176644052167\n",
    "#basic feature + tf_grouped                                 2.8167889079501047\n",
    "#basic feature + tf_fullData                                2.820394158650923\n",
    "#basic feature + tfIdf_grouped                              2.800941096964716\n",
    "#basic feature + tfIdf_fullData_1                           2.823836138011456        \n",
    "#basic feature + tfidf                                      1.8287619171940812\n",
    "#basic feature + sklearn word count                         1.8273230684121524  \n",
    "#(this show that tfidf and sklearn word counttwo are almost equivalent so i just use one of that)\n",
    "#since sklean wordcount is the best i check the possible improvement on that\n",
    "\n",
    "#basic feature + sklearn word count + manual word count     1.8269534098049776    +0.001  +-\n",
    "#basic feature + sklearn word count + word2vect             1.8175582513592656    +0.01   +-\n",
    "#basic feature + sklearn word count + weater                1.8143524722884636    +0.01   +-\n",
    "#basic feature + sklearn word count + weater + word2vect    1.8066107753300489    +0.02   +-\n",
    "#bac fe + skl word ct + wear + word t + manual word count   1.8065145709633865    +0.02   ++-\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My conclusion:\n",
    "\n",
    "\n",
    "both the custom tfidf and tf grouped get a better result that on the all dataset.\n",
    "the sklean wordsount and tfidf are way better that the other feature, and they are very similar\n",
    "\n",
    "both word2vect and weater information add new information on the data,\n",
    "the word2vect add much more infomration that weather on the basic feature, but in the basic information + word count the improvement is almost the same.\n",
    "\n",
    "for the manual wordcount as i sensed add much more infomation (compared to word2vect and weather) on the basic feature, but they add very few information on the basic feature + sklean word count, meaning that they close to be a sub set of infomation of the sklean word count.\n",
    "\n",
    "the best think to do it will be check with a p-test the significance of the improvement obtained.\n",
    "to check if a 0.02 is significant same for 0.01 0.001\n",
    "\n",
    "the best will be also to do the same test as i did with another much comples model\n",
    "to validate it\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
